## Reference
<https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html>

## Structure

Name   |NN1|NN2|DN |ZK |ZKFC|JNN
-------|---|---|---|---|----|---
node01 |*  |   |   |   |*   |*
node02 |   |*  |*  | * |*   |*
node03 |   |   |*  | * |    |*
node04 |   |   |*  | * |    |

## Send ssh pub key from node02 to node01

```bash
    # Generate public key
    ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
    
    # Add the key to localhost
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
    
    # Send the key to node01
    scp id_rsa.pub node01:`pwd`/node02.pub
    # On node01
    cat ~/.ssh/node02.pub >> ~/.ssh/authorized_keys
```

## Configurations of QJM

> hdfs-site.xml

```xml
    <configuration>
        <property>
            <name>dfs.replication</name>
            <value>2</value>
        </property>
        <property>
            <name>dfs.nameservices</name>
            <value>mycluster</value>
        </property>
        <property>
            <name>dfs.ha.namenodes.mycluster</name>
            <value>nn1,nn2</value>
        </property>
        <property>
            <name>dfs.namenode.rpc-address.mycluster.nn1</name>
            <value>node01:8020</value>
        </property>
        <property>
            <name>dfs.namenode.rpc-address.mycluster.nn2</name>
            <value>node02:8020</value>
        </property>
        <property>
            <name>dfs.namenode.http-address.mycluster.nn1</name>
            <value>node01:50070</value>
        </property>
        <property>
            <name>dfs.namenode.http-address.mycluster.nn2</name>
            <value>node02:50070</value>
        </property>
        <property>
            <name>dfs.namenode.shared.edits.dir</name>
            <value>qjournal://node01:8485;node02:8485;node03:8485/mycluster</value>
        </property>
        <property>
            <name>dfs.client.failover.proxy.provider.mycluster</name>
            <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
        </property>
        <property>
            <name>dfs.ha.fencing.methods</name>
            <value>sshfence</value>
        </property>
        <property>
            <name>dfs.ha.fencing.ssh.private-key-files</name>
            <value>/root/.ssh/id_rsa</value>
        </property>
        <property>
            <name>dfs.journalnode.edits.dir</name>
            <value>/var/hadoop/ha/journalnode</value>
        </property>
    </configuration>
```

> core-site.xml

```xml
    <configuration>
        <property>
            <name>fs.defaultFS</name>
            <value>hdfs://mycluster</value>
        </property>
        <property>
            <name>hadoop.tmp.dir</name>
            <value>/var/hadoop/ha</value>
        </property>
    </configuration>
```

### Configurations for ZooKeeper

> hdfs-site.xml

```xml
    <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>
```

> core-site.xml

```xml
    <property>
        <name>ha.zookeeper.quorum</name>
        <value>node02:2181,node03:2181,node04:2181</value>
    </property>
```

### Distribute configuration files to all nodes

```bash
    scp core-site.xml hdfs-site.xml node02:`pwd`
    scp core-site.xml hdfs-site.xml node03:`pwd`
    scp core-site.xml hdfs-site.xml node04:`pwd`
```

## Automatic Failover Setup (ZooKeeper)


> Deploy ZooKeeper on node02

```bash
    tar xf zookeeper-3.4.6.tar.gz -C /opt/
    cd /opt
    mv zookeeper-3.4.6/ zookeeper
    cd zookeeper/conf
    mv zoo_sample.cfg zoo.cfg
```

> Configure ZooKeeper on node02

```bash
    vi zoo.cfg
    
    # Update
    dataDir=/var/ryhz/zk
    
    # Insert
    server.1=node02:2888:3000
    server.2=node03:2888:3000
    server.3=node04:2888:3000
```

> Distribute zookeeper to node03 and node04

```bash
    scp -r  zookeeper/ node03:`pwd`
    scp -r  zookeeper/ node04:`pwd`
```

> Create folder on node 2, 3, 4

```bash
    mkdir -p /var/ryhz/zk
```

> Define myid

```bash
    # node02
    echo 1 > /var/ryhz/zk/myid
    
    # node03
    echo 2 > /var/ryhz/zk/myid
    
    # node04
    echo 3 > /var/ryhz/zk/myid
```

> Set environment variable

```bash
    vi + /etc/profile
    export ZOOKEEPER_HOME=/opt/zookeeper
    PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZOOKEEPER_HOME/bin
    
    # Destribute profile to node03 and node04
    scp /etc/profile node03:/etc/
    scp /etc/profile node04:/etc/
    
    source /etc/profile
```

### Zookeeper operations

At least two nodes should be started, the one with biggest myid is leader, others are followers

> Start, status, stop

```bash
    zkServer.sh start|status|stop
```

## Initializing HA cluster

> Start JournalNode daemons

```bash
    # on node01, 02, 03
    # Hadoop 2.6.5 command:
    hadoop-daemon.sh start journalnode
```

> Format on one namenode

```bash
    # on node01
    hdfs namenode -format
```

> Start formatted namenode

```bash
    # on node02
    hadoop-daemon.sh start namenode
```

> On the unformatted namenode

```bash
    # on node02
    hdfs namenode -bootstrapStandby
```

> Initializing HA state in ZooKeeper

```bash
    # on node01
    hdfs zkfc -formatZK
```

> Start the cluster

```bash
    # on active namenode (node01)
    start-dfs.sh
```

## Some tests

Right now node01 should be active

> Verify from web console

<http://node01:50070>

<http://node02:50070>

> Verify from zookeeper CLI

```bash
    zkCli.sh
    ls /hadoop-ha/mycluster
    get /hadoop-ha/mycluster/ActiveBreadCrumb
    get /hadoop-ha/mycluster/ActiveStandbyElectorLock
```

> Stop active namenode on node01

```bash
    hadoop-daemon.sh stop namenode
```

> Verify again, node02 should be active now

# Namenode HA with QJM is completed set up

> Stop all

```bash
    # on active namenode
    stop-dfs.sh
    
    # on any zookeeper nodes
    zkServer.sh stop
```

> Start all

```bash
    # on ALL zookeeper nodes
    zkServer.sh start
    
    # on active namenode
    start-dfs.sh
```
