## Structure

nodename | NN | SNN | DN 
---|---|---|---
node01 |*| | 
node02 ||*|*
node03 |||*
node04 |||*

## ssh key distribution
> On name node

```bash
    cd .ssh (if file not exist, run "ssh localhost")
    scp id_rsa.pub node02:`pwd`/node01.pub
    scp id_rsa.pub node03:`pwd`/node01.pub
    scp id_rsa.pub node04:`pwd`/node01.pub
```

> On other nodes

    cd .ssh
    cat node01.pub >> authorized_keys
    
# Name Node
## Configurations
> /opt/hadoop/etc/hadoop/core-site.xml

```xml
    <configuration>
        <property>
            <name>fs.defaultFS</name>
            <value>hdfs://node01:9000</value>
        </property>
        <property>
            <name>hadoop.tmp.dir</name>
            <value>/var/hadoop/full</value>
        </property>
    </configuration>
```

> /opt/hadoop/etc/hadoop/hdfs-site.xml

```xml
    <configuration>
        <property>
            <name>dfs.replication</name>
            <value>2</value>
        </property>
        <property>
            <name>dfs.namenode.secondary.http-address</name>
            <value>node02:50090</value>
        </property>
    </configuration>
```

> /opt/hadoop/etc/hadoop/slaves

    node02
    node03
    node04

## Copy hadoop folder to other nodes
On node01, run below commands:

```bash    
    cd /opt
    scp -r hadoop/ node02:`pwd`
    scp -r hadoop/ node03:`pwd`
    scp -r hadoop/ node04:`pwd`
    
    # Environment Variables
    scp /etc/profile node02:/etc
    scp /etc/profile node03:/etc
    scp /etc/profile node04:/etc
```

## Hadoop operations

> Format

```bash
    hdfs namenode -format
```

> Start namenode

```bash
    start-dfs.sh 
```

```log
        Starting namenodes on [node01]
        node01: starting namenode, logging to /opt/hadoop/logs/hadoop-root-namenode-node01.out
        node03: starting datanode, logging to /opt/hadoop/logs/hadoop-root-datanode-node03.out
        node02: starting datanode, logging to /opt/hadoop/logs/hadoop-root-datanode-node02.out
        node04: starting datanode, logging to /opt/hadoop/logs/hadoop-root-datanode-node04.out
        Starting secondary namenodes [node02]
        node02: starting secondarynamenode, logging to /opt/hadoop/logs/hadoop-root-secondarynamenode-node02.out
```

> Create root folder

```bash
    hdfs dfs -mkdir -p /user/root
```

> Web console

<http://node01:50070>

> Set block size while uploading

```bash
    # Create a small file around 1.8M
    for i in `seq 150000`;do echo "hello world" >> test.txt;done
    
    # Upload to dfs, in the meanwhile set block size (default=128M)
    # 1048576 Bytes = 1M
    hdfs dfs -D dfs.blocksize=1048576 -put test.txt
```